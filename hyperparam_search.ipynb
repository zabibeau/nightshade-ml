{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28ef9ac3",
   "metadata": {},
   "source": [
    "# Finding hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ff31349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import clip\n",
    "from torchvision import transforms\n",
    "from skimage.metrics import structural_similarity as ssim, peak_signal_noise_ratio as psnr\n",
    "from perturbation_methods import fgsm_penalty, pgd_penalty, nightshade_penalty\n",
    "from nightshade import Nightshade\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17018745",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m DEVICE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m CLIP_MODEL, CLIP_PREPROCESS \u001b[38;5;241m=\u001b[39m \u001b[43mclip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mViT-B/32\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Spring2025/MachineLearning/nightshade-ml/nightshade_env/lib/python3.10/site-packages/clip/clip.py:120\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, device, jit, download_root)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a CLIP model\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    A torchvision transform that converts a PIL image into a tensor that the returned model can take as its input\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _MODELS:\n\u001b[0;32m--> 120\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m \u001b[43m_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MODELS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpanduser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m~/.cache/clip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(name):\n\u001b[1;32m    122\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m name\n",
      "File \u001b[0;32m~/Spring2025/MachineLearning/nightshade-ml/nightshade_env/lib/python3.10/site-packages/clip/clip.py:54\u001b[0m, in \u001b[0;36m_download\u001b[0;34m(url, root)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdownload_target\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m exists and is not a regular file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(download_target):\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mhashlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msha256\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdownload_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mhexdigest() \u001b[38;5;241m==\u001b[39m expected_sha256:\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m download_target\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CLIP_MODEL, CLIP_PREPROCESS = clip.load(\"ViT-B/32\", device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d83284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 images from the dataset.\n"
     ]
    }
   ],
   "source": [
    "def load_subset(path, max_samples=10):\n",
    "    files = [f for f in sorted(os.listdir(path)) if f.endswith(\".p\")][:max_samples]\n",
    "    subset = []\n",
    "    for f in files:\n",
    "        with open(os.path.join(path, f), 'rb') as pf:\n",
    "            data = pickle.load(pf)\n",
    "            subset.append(data)\n",
    "    return subset\n",
    "\n",
    "images = load_subset('poisoning_candidates/pickle', max_samples=20)\n",
    "print(f\"Loaded {len(images)} images from the dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22f14fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clip_similarity(image, text):\n",
    "    image_tensor = CLIP_PREPROCESS(image).unsqueeze(0).to(DEVICE)\n",
    "    text_tokens = clip.tokenize([text]).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        image_features = CLIP_MODEL.encode_image(image_tensor)\n",
    "        text_features = CLIP_MODEL.encode_text(text_tokens)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    return (image_features @ text_features.T).item()\n",
    "\n",
    "def evaluate_attack(original_img, poisoned_img, target_text):\n",
    "    # Convert to arrays\n",
    "    orig_arr = np.array(original_img.resize((224, 224)))\n",
    "    poison_arr = np.array(poisoned_img.resize((224, 224)))\n",
    "\n",
    "    # Metrics\n",
    "    clip_score_target = compute_clip_similarity(poisoned_img, target_text)\n",
    "    clip_score_original = compute_clip_similarity(poisoned_img, \"\")  # empty prompt for base concept\n",
    "    l2_dist = np.linalg.norm(poison_arr.astype(np.float32) - orig_arr.astype(np.float32))\n",
    "    ssim_val = ssim(orig_arr, poison_arr, channel_axis=-1, data_range=255)\n",
    "    psnr_val = psnr(orig_arr, poison_arr, data_range=255)\n",
    "\n",
    "    return {\n",
    "        \"clip_target\": clip_score_target,\n",
    "        \"clip_original\": clip_score_original,\n",
    "        \"l2\": l2_dist,\n",
    "        \"ssim\": ssim_val,\n",
    "        \"psnr\": psnr_val\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6b4446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f086c930780144b194fba517047e4850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    }
   ],
   "source": [
    "fgsm_eps = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "pgd_eps = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "pgd_step_size = [0.01, 0.05, 0.1, 0.2]  \n",
    "pgd_iterations = [5, 10, 20, 50, 100]\n",
    "\n",
    "nightshade_eps = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "nightshade_iterations = [5, 10, 20, 50, 100, 150, 200]\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None,\n",
    ").to(DEVICE)\n",
    "pipe._progress_bar_config={\"disable\": True}\n",
    "target_text = 'cat'\n",
    "\n",
    "ns = Nightshade('cat', DEVICE, None, pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012a97ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d47e9828c2548768c9bb41febdc6317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b481a18eac8849a295345f6eefbdb11c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e778275079cf48139a09714147c61a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m eps \u001b[38;5;129;01min\u001b[39;00m fgsm_eps:\n\u001b[1;32m      8\u001b[0m     ns\u001b[38;5;241m.\u001b[39meps \u001b[38;5;241m=\u001b[39m eps\n\u001b[0;32m----> 9\u001b[0m     poisoned_img \u001b[38;5;241m=\u001b[39m \u001b[43mns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m evaluate_attack(original_img, poisoned_img, target_text)\n\u001b[1;32m     11\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend((eps, metrics))\n",
      "File \u001b[0;32m~/Spring2025/MachineLearning/nightshade-ml/nightshade.py:89\u001b[0m, in \u001b[0;36mNightshade.generate\u001b[0;34m(self, img, target_concept)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, img, target_concept):\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Generate target image\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 89\u001b[0m         target_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mA photo of a \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtarget_concept\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     resized_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# Source and target tensor\u001b[39;00m\n",
      "File \u001b[0;32m~/Spring2025/MachineLearning/nightshade-ml/nightshade.py:111\u001b[0m, in \u001b[0;36mNightshade.generate_target\u001b[0;34m(self, prompt, seed)\u001b[0m\n\u001b[1;32m    109\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 111\u001b[0m     target_imgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msd_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mimages\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m target_imgs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Spring2025/MachineLearning/nightshade-ml/nightshade_env/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Spring2025/MachineLearning/nightshade-ml/nightshade_env/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:1060\u001b[0m, in \u001b[0;36mStableDiffusionPipeline.__call__\u001b[0;34m(self, prompt, height, width, num_inference_steps, timesteps, sigmas, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, output_type, return_dict, cross_attention_kwargs, guidance_rescale, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     noise_pred \u001b[38;5;241m=\u001b[39m rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mguidance_rescale)\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;66;03m# compute the previous noisy sample x_t -> x_t-1\u001b[39;00m\n\u001b[0;32m-> 1060\u001b[0m latents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_step_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callback_on_step_end \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1063\u001b[0m     callback_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/Spring2025/MachineLearning/nightshade-ml/nightshade_env/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:257\u001b[0m, in \u001b[0;36mPNDMScheduler.step\u001b[0;34m(self, model_output, timestep, sample, return_dict)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_prk(model_output\u001b[38;5;241m=\u001b[39mmodel_output, timestep\u001b[38;5;241m=\u001b[39mtimestep, sample\u001b[38;5;241m=\u001b[39msample, return_dict\u001b[38;5;241m=\u001b[39mreturn_dict)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_plms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Spring2025/MachineLearning/nightshade-ml/nightshade_env/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:382\u001b[0m, in \u001b[0;36mPNDMScheduler.step_plms\u001b[0;34m(self, model_output, timestep, sample, return_dict)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    380\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m24\u001b[39m) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m55\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mets[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m59\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mets[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m37\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mets[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m9\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mets[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m])\n\u001b[0;32m--> 382\u001b[0m prev_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_prev_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_timestep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/Spring2025/MachineLearning/nightshade-ml/nightshade_env/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:420\u001b[0m, in \u001b[0;36mPNDMScheduler._get_prev_sample\u001b[0;34m(self, sample, timestep, prev_timestep, model_output)\u001b[0m\n\u001b[1;32m    418\u001b[0m alpha_prod_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphas_cumprod[timestep]\n\u001b[1;32m    419\u001b[0m alpha_prod_t_prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphas_cumprod[prev_timestep] \u001b[38;5;28;01mif\u001b[39;00m prev_timestep \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_alpha_cumprod\n\u001b[0;32m--> 420\u001b[0m beta_prod_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43malpha_prod_t\u001b[49m\n\u001b[1;32m    421\u001b[0m beta_prod_t_prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alpha_prod_t_prev\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mprediction_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv_prediction\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Spring2025/MachineLearning/nightshade-ml/nightshade_env/lib/python3.10/site-packages/torch/_tensor.py:37\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f, assigned\u001b[38;5;241m=\u001b[39massigned)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;66;03m# See https://github.com/pytorch/pytorch/issues/75462\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mhas_torch_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     38\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# FGSM\n",
    "with open(\"fgsm_results.txt\", \"w\") as f:\n",
    "    results = []\n",
    "    for image_data in tqdm(images, desc=\"Images\"):\n",
    "        original_img = Image.fromarray(image_data['img'])\n",
    "        ns.penalty_method = fgsm_penalty\n",
    "        for eps in tqdm(fgsm_eps, description=\"FGSM Epsilon\", postfix=eps):\n",
    "            ns.eps = eps\n",
    "            poisoned_img = ns.generate(original_img, target_text)\n",
    "            metrics = evaluate_attack(original_img, poisoned_img, target_text)\n",
    "            results.append((eps, metrics))\n",
    "        \n",
    "    # Average the results for each epsilon\n",
    "    avg_results = {}\n",
    "    for eps, metrics in results:\n",
    "        if eps not in avg_results:\n",
    "            avg_results[eps] = {k: [] for k in metrics.keys()}\n",
    "        for k, v in metrics.items():\n",
    "            avg_results[eps][k].append(v)\n",
    "    for eps, metrics in avg_results.items():\n",
    "        avg_results[eps] = {k: np.mean(v) for k, v in metrics.items()}\n",
    "        f.write(f\"FGSM Epsilon: {eps}\\n\")\n",
    "        for k, v in avg_results[eps].items():\n",
    "            f.write(f\"{k}: {v}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9a365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PGD\n",
    "with open('pgd_results.txt', 'w') as f:\n",
    "    results = []\n",
    "    for image_data in tqdm(images):\n",
    "        original_img = Image.fromarray(image_data['img'])\n",
    "        ns.penalty_method = pgd_penalty\n",
    "        for eps in tqdm(pgd_eps, desc=\"PGD Epsilon\", postfix=eps):\n",
    "            for step_size in tqdm(pgd_step_size, desc=\"PGD Step Size\", postfix=step_size):\n",
    "                for iterations in tqdm(pgd_iterations, desc=\"PGD Iterations\", postfix=iterations):\n",
    "                    ns.eps = eps\n",
    "                    ns.step_size = step_size\n",
    "                    ns.iterations = iterations\n",
    "                    poisoned_img = ns.generate(original_img, target_text)\n",
    "                    metrics = evaluate_attack(original_img, poisoned_img, target_text)\n",
    "                    results.append((eps, step_size, iterations, metrics))\n",
    "        \n",
    "    # Average the results for each combination of parameters\n",
    "    avg_results = {}\n",
    "    for eps, step_size, iterations, metrics in results:\n",
    "        key = (eps, step_size, iterations)\n",
    "        if key not in avg_results:\n",
    "            avg_results[key] = {k: [] for k in metrics.keys()}\n",
    "        for k, v in metrics.items():\n",
    "            avg_results[key][k].append(v)\n",
    "    for key, metrics in avg_results.items():\n",
    "        avg_results[key] = {k: np.mean(v) for k, v in metrics.items()}\n",
    "        f.write(f\"PGD Epsilon: {key[0]}, Step Size: {key[1]}, Iterations: {key[2]}\\n\")\n",
    "        for k, v in avg_results[key].items():\n",
    "            f.write(f\"{k}: {v}\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec780212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Nightshade penalty\n",
    "with open('nightshade_results.txt', 'w') as f:\n",
    "    results = []\n",
    "    for image_data in tqdm(images):\n",
    "        original_img = Image.fromarray(image_data['img'])\n",
    "        ns.penalty_method = nightshade_penalty\n",
    "        for eps in tqdm(nightshade_eps, desc=\"Nightshade Epsilon\", postfix=eps):\n",
    "            for iterations in tqdm(nightshade_iterations, desc=\"Nightshade Iterations\", postfix=iterations):\n",
    "                ns.eps = eps\n",
    "                ns.iterations = iterations\n",
    "                poisoned_img = ns.generate(original_img, target_text)\n",
    "                metrics = evaluate_attack(original_img, poisoned_img, target_text)\n",
    "                results.append((eps, iterations, metrics))\n",
    "        \n",
    "    # Average the results for each combination of parameters\n",
    "    avg_results = {}\n",
    "    for eps, iterations, metrics in results:\n",
    "        key = (eps, iterations)\n",
    "        if key not in avg_results:\n",
    "            avg_results[key] = {k: [] for k in metrics.keys()}\n",
    "        for k, v in metrics.items():\n",
    "            avg_results[key][k].append(v)\n",
    "    for key, metrics in avg_results.items():\n",
    "        avg_results[key] = {k: np.mean(v) for k, v in metrics.items()}\n",
    "        f.write(f\"Nightshade Epsilon: {key[0]}, Iterations: {key[1]}\\n\")\n",
    "        for k, v in avg_results[key].items():\n",
    "            f.write(f\"{k}: {v}\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nightshade_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
